export const metadata = {
  title: "The Future We're Building",
  subtitle: "The future of AI development",
  author: "Chris Maresca",
  date: "June 10, 2025",
  category: "Company",
  excerpt: "Astral's vision for the future of AI development.",
  heroImage: "/resources/vision.png",
}

Artificial intelligence is going to redefine what it means to be human. 

I think in a positive way.

#### But we need two things to get there.

First, we need to make it dead simple to build intelligent systems<FootnoteRef id="1" />.  That means reducing the number of moving parts and perfecting the few that actually matter.

Second, we need to design a developer experience so focused that builders can spend nearly all their time improving intelligence and as little time as possible on everything else.

This is what AGI will actually look like. Not a single model, at least not any time soon. It will come from perfecting the foundation for building intelligent systems, maximizing the time builders spend using it, and eventually enabling those systems to improve themselves.

Astral exists to make that future possible. We are rebuilding the AI development stack from first principles. To do it right, we have to understand what is broken and rethink what is needed to support this new way of building.

### The Experience Today

This way of thinking started long before Astral. I spent the last 18 months building intelligent systems for startups. Every project felt the same. I was spending more time wrestling with tools than building anything useful.

The problem wasn't the tools themselves. It was getting them to work together. I was using four different services to call models, rewriting integrations to reach external systems, juggling multiple tracing setups, and still struggling to get anything running in production. Nothing worked end-to-end.

Eventually I got tired of it. So I started building my own tools. Not to start a company. Just to make my own work easier.

Each tool was small, but they all spoke the same language. I didn't realize it at the time, but I was starting to answer a deeper question. *What are the actual components needed to build intelligent systems?*

Once the tools came together, I was building in hours what was taking other teams weeks. I started getting pulled into more projects, mostly with YC startups. A few asked me to join full time. I turned down four offers, including a CTO role, to keep exploring what these tools could become.

Eventually I joined 1Mind, where I had the freedom to keep building. The systems I built became core to how the company operated and helped drive it to $1 million in ARR.

That's when I realized it wasn't just me. Everyone building intelligent systems was hitting the same wall. You only get two bad options.

The first is to stitch together a pile of fragmented tools like I was doing. None of them work well together. And as the system grows, the complexity only gets worse.

The second is to use a platform from a big vendor like IBM or Microsoft. They give you some unification, but at the cost of flexibility and control.

### Rethinking the Stack

If we want intelligent systems that can build and improve themselves, we have to start from first principles. What does it actually take to build durable, high-performing intelligence?

Surprisingly little. Just three core layers and one capability that runs through all of them.

#### **The Model Layer**

This layer is pretty straightforward. Models are the intelligence in intelligent systems. They take in context and produce an output. Everything else exists to support that process. So it makes sense to start here.

What's not so simple is what you need from this layer. 

First, you need to be able to use any model from any provider. Different models are good at different things. Some excel at reasoning. Some at coding. Others are finetuned on your own data. We see models as a new form of talent. The best developers will reach for the best model for the task, which is often one that just launched. So you also need instant access to every new model release.

Second, you need a standard way to connect models to external systems. That is where the Model Context Protocol<FootnoteRef id="2" /> comes in. I won't get into the weeds, but MCP gives you consistency in how models access tools and data. It eliminates custom integrations and lets systems stay simple as they scale.

Third, you need resilience. Providers go down. Latency spikes. But the system still has to work. You should be able to route around issues automatically, the same way you would with any reliable infrastructure.

#### **The Orchestration Layer**

Once you can use any model, the next question is how you use them. That is the job of the orchestration layer.

Orchestration is about structure. You take intelligent decisions and put them into some repeatable form. We see these fall into two buckets: workflows and agents.

It is important to keep a clear distinction between the two, both in how you think about them and how you implement them. The clearest thinking I've seen on this comes from Anthropic, who [stress the importance of keeping workflows and agents distinct](https://www.anthropic.com/engineering/building-effective-agents). I agree.

The key is to keep things simple and composable. Workflows and agents should both build on the same model layer. Sharing a common foundation makes systems easier to understand, debug, and scale.

Most frameworks don't get this right. They bury logic across abstracted layers or lock you into a single provider<FootnoteRef id="3" />. That may work for prototypes, but it limits you as systems grow. We share Anthropic's view: "*frameworks often introduce layers that obscure the core prompts and responses, making them harder to debug*."<FootnoteRef id="4" />

This is where simplicity earns its keep. The fewer moving parts a system has, and the more they're built on a shared foundation, the easier it is for other intelligent systems to understand it and improve it.

#### **The Deployment Layer**

Once you can build intelligent systems, you need a way to run them. That's the role of the deployment layer.

Deployment should be simple. One command should take you from a prototype to a running system. Think of the experience Vercel brought to web applications, but applied to intelligent systems.

But simplicity should not come at the cost of flexibility. You should be able to run your code anywhere. That includes serverless platforms like AWS Lambda, containers on GCP or Azure, or on-prem in your own environment.

Finally, deployment should fit naturally into how teams already work. That means integration with popular version control platforms. You should be able to push changes, track history, and roll back when needed.

#### **Observability And Evaluation**

Observability is the feedback loop that runs through the entire stack. Intelligent systems behave less like traditional software and more like people. But unlike people, they need constant observation. You need to see every decision they make: what happened, when it happened, and what context it had at the time.

You also need to experiment constantly. Try new prompts. Swap in different models. Add or remove tools. Progress depends on the speed of iteration.

Finally, you need to track usage, cost, and performance. As systems scale, so do the risks. You need a clear view of what is running, what it is costing, and whether it is creating value.

### From Principles to Practice

Our goal is to make it dead simple to build intelligent systems. The simpler it is, the more likely we reach a future where those systems can build and improve themselves. We've now walked through the raw ingredients needed to get there. But how that thinking gets implemented is still far from obvious.

So let me rewind. As I mentioned earlier, the tools I'm about to walk through didn't start with the goal of becoming a platform. They started as a way to make my own work easier. Each one solved a real problem I kept hitting while building intelligent systems for startups. I just needed things to work better, and once they did, I realized the potential of what I was building.

Each tool does one thing exceptionally well. Each maps to a clear part of the stack. And most importantly, they all speak the same language. That shared foundation is what keeps the system simple. It's what lets builders spend their time improving intelligence and what opens the door to systems that can eventually build and improve themselves.

#### **Astral Core**

This is our model layer. It is a Python library that gives you one interface to any model, from any provider. The library implements the Model Context Protocol to standardize how models access tools and data. It's also capable of routing around provider outages and latency spikes. Other routers exist, but most wrap legacy APIs, few support MCP, and almost none offer a clean, provider-style developer experience. And most importantly, none work well with the rest of the stack.

#### **Astral Agents SDK and Astral Workflows SDK**

This is our orchestration layer. Earlier, we stressed how important it is to keep agents and workflows distinct, and to build them on a shared foundation. These SDKs do exactly that. Each offers a simple interface for its own kind of system. Both follow the same structure and syntax because both are built on Astral Core. That makes them easier to understand, easier to debug, and easier to work across. Most agent SDKs are heavy with abstraction or only avoid it when tied to a single provider. Ours stay minimal and consistent.

#### **Astral Deployment**

This moves intelligent systems into production. Today it's a small set of configurable Infrastructure as Code (IaC) scripts for AWS. It'll grow into a full deployment layer that runs on any runtime, from serverless to containers to your own machines. Hardly any python tools treat deployment as a real layer for intelligent systems, and none do it while supporting everything underneath.

#### **Astral Observability**

This is visibility across every layer. You see every model call, agent decision, and workflow step in one place. You also get usage, latency, and performance. A lot of teams are building observability frameworks, but they're doing it as outsiders, which means you need to constantly rework things every time a layer changes. This is the advantage of building the stack from the ground up. The best part is, it's not even a true separate component. It's built into the three layers above. There's nothing extra to install. And if you don't want it, just set an environment variable to turn it off.

Our true competitive advantage is the consistent language and shared primitives across every layer. Hardly any tools try to solve the full stack. Most are open source, which makes coordination across layers nearly impossible. The rest come from providers who aren't incentivized to fully support anything beyond their own models. We're building the system that ties everything together.

### How We'll Get There

So the question now is how we bring it to life. We're taking a three-phase path to get there.

First, we're building for others. We're working closely with teams to design and deploy intelligent systems using the internal tools we outlined above. This gives us instantaneous feedback loops and proves the leverage the system creates.

Next, we'll turn those tools into a unified platform. One that gives developers and teams everything they need to build, run, and evolve intelligent systems.

And once the platform is mature, we'll use it to build the machine that builds the machine. We'll give every developer and team the equivalent of a dedicated AI engineering team. This is the long-term goal. A compounding engine of intelligence, built from simple and modular parts.

#### **Phase One: Implementation**

Right now, we're using our own SDKs to build workflows and agents for companies. It doesn't scale forever, but that's the point. I'm a firm believer in [doing things that don't scale](https://www.paulgraham.com/ds.html) early on. The problem we're solving is hard, and being the first and heaviest users means our feedback loop is instantaneous.

This also lets us prove how much leverage the system creates. We’ve already signed a deal with a late-stage startup to build and maintain a custom AI workflow. I built the initial version in a single day, working solo. The contract covers both implementation and ongoing maintenance, and the time required each week is minimal. The return on time is unlike anything I’ve seen in software.

The goal now is to close 3-4 more of these implementation deals. We're targeting late stage startups and mid market software companies, using warm intros and targeted outreach to get in the door. 

At the same time, we're raising a $1 million initial round. That gets us our first office and locks in a founding team of two engineers and one exceptional generalist. I'm firm in my belief that building a startup is very difficult. Trying to do it remote only makes it harder. I want our founding team in the same room.

#### **Phase Two: Platform**

During our early implementation deals, we'll package our internal tools into the Astral platform. It will include our core Python library, two SDKs for agents and workflows, and a single CLI for managing resources, deployment, and observability. On top of that, we'll offer a hosted console for deploying and managing intelligent systems. Think of the experience Vercel created for web apps, but applied to AI.

From here, we're going to have a relentless obsession with developer experience. Everything else will work backwards from that. But what does that actually mean?

First, we're going to have the simplest quick start in the industry. A clear, step-by-step flow to build and deploy your first workflow or agent with Astral. Backed by world-class documentation, end-to-end guides, and walkthroughs. This is what modern developers expect, and we're going to surpass it.

We're also going to live in developer spaces, both online and in person. Hackathons, events, Reddit threads, Discord groups. Anywhere builders are, we'll be there too. From there, we'll grow our own developer community. It will start on Discord and expand through real-world events. Me and the early team will be in it every day. Answering questions. Fixing bugs. Sharing progress. Obsessively active.

Exceptional design is still an early thought in this space, but already proven to be a real advantage. Look at Linear, Vercel, Next.js, ShadCN, and Motion. They win because of how their products feel. Astral will be the same. We'll start with how we want it to feel and build backwards from there.

#### **Phase Three: The Machine That Builds the Machine**

Once the platform is stable, we'll use it ourselves to build agents and workflows that can improve other intelligent systems. This is the flywheel we're building toward and it gives us leverage on both sides of the business. 

On the platform, it lets us layer intelligence on top of existing usage. Just like Notion did with Notion AI. We start with infrastructure buy-in, then add systems that help you build and operate more effectively. This increases our recurring revenue from existing customers.

In implementation, it compounds what we already have. We've proven we can sell high-leverage systems. This only improves margins and shortens delivery time. Put simply, this is the worst the Astral system will ever be. Everything from here gets faster and more intelligent.

It also opens new markets. Once we have the platform and intelligence, we can deploy our own agents directly to businesses and consumers. These agents become products in their own right. Think of it as a long-term engine for launching new workflows and capabilities, powered by everything we've already built.

### Where We're Headed

We're going to move fast but carefully. We could have built a vertical agent that generates other agents. It would have been faster to ship. But we're choosing the harder path. **Rebuilding the stack from the ground up gives us immense long-term leverage**.

It's worth repeating. We're setting out to make it dead simple to build intelligent systems. That's how we reach a future where those systems can build and improve themselves. Everything we are building — our libraries, our platform, and our path — is in service of that goal.

Thank you for reading.

<FootnoteSection>
  <Footnote id="1">We define intelligent systems primarily as workflows and agents, which are defined well by Anthropic in footnote 4.</Footnote>
  <Footnote id="2">Read more here: <a href="https://modelcontextprotocol.io/introduction">https://modelcontextprotocol.io/introduction</a></Footnote>
  <Footnote id="3">Even Google and OpenAI's SDKs fall short unless you commit to their entire ecosystem</Footnote>
  <Footnote id="4"><a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></Footnote>
</FootnoteSection>